{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23217184"
      },
      "source": [
        "In this week's tutorial we will look into generative modelling. At the end of this tutorial you should be able to:\n",
        "- Build a variational auto-encoder\n",
        "- Use it to generate images\n",
        "- Understand the effect of the loss on the latent encoding of images\n",
        "\n",
        "**If possible, use a GPU to train the models, especially for the convolutional VAE which otherwise could take too long to train (if you don't have a GPU you can try to run on Google Colab).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intuition behind auto-encoders\n",
        "Auto-encoders are a type of neural network that tries to learn a compressed representation of the input data. The network is composed of two parts: an encoder and a decoder. The encoder takes the input data and compresses it into a latent representation in $\\mathbb{R}^d$. The decoder takes the latent representation and tries to reconstruct the original input data. The network is trained by minimizing the difference between the input data and the reconstructed data so that Decoder $\\circ$ Encoder $\\approx$ Identity.\n",
        "\n",
        "We will then use the latent representation to generate new data. The idea is to sample a latent representation from a prior distribution and pass it through the decoder to generate new data. Here, we will use a Gaussian prior distribution and sample $x\\in\\R^d\\sim \\mathcal{N}(0,I_d)$ and outputs $x_{\\text{gen}} = \\text{Decoder}(x)$.\n",
        "\n",
        "You can have a quick look at the following blogpost for a more detailed explanation: https://lilianweng.github.io/posts/2018-08-12-vae/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e9227ab"
      },
      "source": [
        "# Building a variational auto-encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_rZLfWdFwbw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAr2exvwgLW1"
      },
      "source": [
        "For this section, we will begin by working on the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuI89wgRF0km",
        "outputId": "fa36aac8-1c72-4477-f66f-8857722acc8f"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZuPKN_fgnl5"
      },
      "source": [
        "Let's visualise one of the images in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "TebgLC-fqjsp",
        "outputId": "b90dff7c-16ea-4f1c-cdcc-c47546654883"
      },
      "outputs": [],
      "source": [
        "# get some random training images; this will load a set of images of size batchsize\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "img = images[0] # take just the first image; the above thing is an array of images of size batchsize\n",
        "\n",
        "# Print the original image\n",
        "npimg = img.numpy() # translate to numpy\n",
        "npimg_t = np.transpose(npimg, (1, 2, 0))\n",
        "plt.imshow(npimg_t, cmap='gray')\n",
        "plt.title(str(classes[labels[0]]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtiG7P75guT6"
      },
      "source": [
        "Let's now define the architecture for the VAE. Let $p^*(\\mathbf{x})$ be the true distribution from which images $\\mathbf{x}$ are sampled. We want to find an encoding of $\\mathbf{x}$ as a latent variable $\\mathbf{z}$ (where the dimension of $\\mathbf{z}$ is less than the dimension of $\\mathbf{x}$) which we will choose to sample from $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$.\n",
        "\n",
        "The model will involve two parts: the encoder network parameterising $q_\\psi(\\mathbf{z}|\\mathbf{x})$ and the decoder network parameterising $p_\\phi(\\mathbf{x}|\\mathbf{z})$.\n",
        "\n",
        "**Encoder**\n",
        "\n",
        "We will assume that, for each image $\\mathbf{x}$ we input to the encoder, $q_\\psi(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu_x}, \\text{diag}(\\boldsymbol{\\sigma_x}))$. Hence, we define a network which reduces the dimension of the output at each layer and eventually outputs the mean $\\boldsymbol{\\mu_x}$ and the covariance matrix $\\text{diag}(\\boldsymbol{\\sigma_x})$. We typically assume that the covariance matrix is diagonal and that the outputted $\\boldsymbol{\\sigma_x}$ is the vector containing the entries of the diagonal instead of parameterising all the elements of the covariance matrix.\n",
        "\n",
        "**Reparametrisation**\n",
        "\n",
        "We will then need to sample from $\\mathcal{N}(\\boldsymbol{\\mu_x}, \\boldsymbol{\\sigma_x})$, which we do by $\\mathbf{z} = \\boldsymbol{\\mu_x} + \\boldsymbol{\\sigma_x} \\odot \\boldsymbol{\\epsilon}$, where $\\odot$ denotes element-wise product and $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$.\n",
        "\n",
        "**Decoder**\n",
        "\n",
        "We will define a network which will increase the dimension of the output at each layer and eventually outputs a vector of the same dimensions as the input image $\\mathbf{x}$ passed through a sigmoid activation function.\n",
        "\n",
        "Since the values of the pixels in the MNIST dataset are in the range $[0,1]$ and tend to be binary (close to either $0$ or $1$), we can assume the distribution of the pixel values to be Bernoulli. Hence, we choose\n",
        "$$p_\\phi(\\mathbf{x}|\\mathbf{z}) =  \\prod_{i=1}^D y_i^{x_i}(1-y_i)^{1-x_i},$$\n",
        "$$\\log p_\\phi(\\mathbf{x}|\\mathbf{z}) = \\sum_{i=1}^D x_i \\log y_i + (1-x_i) \\log (1-y_i),$$\n",
        "where $y_i$ is the output of the decoder and $x_i$ the pixel value of the original image. We will also use the value of $y_i$ as the output image (recall that this is also the mean in a Bernoulli distribution).\n",
        "\n",
        "Note that we have assumed that the MNIST dataset is binary, when this is not the case, however even with this assumption, models has been shown to produce good quality images, and most models of VAEs working on this dataset have been implemented using this assumption. See https://proceedings.neurips.cc/paper_files/paper/2019/file/f82798ec8909d23e55679ee26bb26437-Paper.pdf for more details and an approach that provides an alternative that is probabilistically coherent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjkyglpQqt1B"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=2, input_dim=784):\n",
        "        \"\"\"builds VAE\n",
        "        Inputs:\n",
        "            - latent_dim: dimension of latent space\n",
        "            - input_dim: dimension of input space\n",
        "        \"\"\"\n",
        "        super(VAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder layers\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.encoder_mean = nn.Linear(128, latent_dim)\n",
        "        self.encoder_logvar = nn.Linear(128, latent_dim)\n",
        "\n",
        "        # Decoder layers\n",
        "        self.fc3 = nn.Linear(latent_dim, 128)\n",
        "        self.fc4 = nn.Linear(128, 512)\n",
        "        self.fc5 = nn.Linear(512, 784)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"take an image, and return latent space mean and log variance\n",
        "        Inputs:\n",
        "            -x: batch of images flattened to 784\n",
        "        Outputs:\n",
        "            -means in latent dimension\n",
        "            -logvariances in latent dimension\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.encoder_mean(x), self.encoder_logvar(x) # We return mu_x and log_sigma_x\n",
        "\n",
        "    def reparametrise(self, mu, logvar):\n",
        "        \"\"\"Sample in latent space according to mean and logvariance\n",
        "        Inputs:\n",
        "            -mu: batch of means\n",
        "            -logvar: batch of logvariances\n",
        "        Outputs:\n",
        "            -samples: batch of latent samples\n",
        "        \"\"\"\n",
        "        # Fill in - note we chose to work with the logvariances instead of the variances in the encode function\n",
        "        # return sample from N(mu, sigma^2) = mu + sigma * epsilon, with sigma = exp(logvar/2)\n",
        "        return samples\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"Decode latent space samples\n",
        "        Inputs:\n",
        "            -z: batch of latent samples\n",
        "        Outputs:\n",
        "            -x_recon: batch of reconstructed images\n",
        "        \"\"\"\n",
        "        # Fill in - remember to pass the raw output of the model through a Sigmoid function to ensure pixel values are in [0,1]\n",
        "        # Define an MLP with the 3 decoder layers with ReLU activations for the hidden layers and a Sigmoid activation for the output layer\n",
        "        return x_recon\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Do full encode and decode of images\n",
        "        Inputs:\n",
        "            - x: batch of images\n",
        "        Outputs:\n",
        "            - self.decode(z): batch of reconstructed images\n",
        "            - mu: batch of latent mean values\n",
        "            - logvar: batch of latent logvariances\n",
        "        \"\"\"\n",
        "        mu, logvar = self.encode(x.view(-1, 784)) # first reshape image into 1d vector\n",
        "        z = self.reparametrise(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DookNa8qxnm"
      },
      "source": [
        "**Loss**\n",
        "\n",
        "The loss of a VAE, also known as the ELBO loss, is given by\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}, \\phi, \\psi) = KL(q_\\psi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z})) - \\mathbb{E}_{\\mathbf{z} \\sim q_\\psi(\\mathbf{z}|\\mathbf{x})} \\log p_\\phi(\\mathbf{x}|\\mathbf{z}).\n",
        "$$\n",
        "\n",
        "We chose $q_\\psi(\\mathbf{z}|\\mathbf{x})$ and $p(\\mathbf{z})$ to be Gaussians, so the KL divergence can be computed in close form as\n",
        "$$\n",
        "KL(q_{\\psi}(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z})) = \\frac{1}{2}\\left(-\\sum_{j=1}^J\\log \\sigma_j^2 - J + \\sum_{j=1}^J \\sigma_j^2 + \\sum_{j=1}^J\\mu_j^2\\right).\n",
        "$$\n",
        "\n",
        "Taking the expectation with a Monte Carlo approximation using one sample, we can write the remaining term as\n",
        "$$\n",
        "-\\log p_\\phi(\\mathbf{x}|\\mathbf{z}) = - \\sum_{i=1}^D x_i \\log y_i + (1-x_i) \\log (1-y_i).\n",
        "$$\n",
        "This last term can be computed using F.binary_cross_entropy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ApNJRdcq1PS"
      },
      "outputs": [],
      "source": [
        "def ELBO_loss(x, reconstructed_x, mu, logvar):\n",
        "    '''\n",
        "    calculates ELBO loss\n",
        "    Inputs:\n",
        "        - x: batch of images\n",
        "        - reconstructed_x: batch of reconstructed images\n",
        "        - mu: batch of latent mean values\n",
        "        - logvar: batch of latent logvariances\n",
        "    Outputs:\n",
        "        - neg_loglikelihood: average value of negative log-likelihood term across batch\n",
        "        - KL_divergence: average value of KL divergence term across batch\n",
        "        - loss: average ELBO loss across batch\n",
        "    '''\n",
        "    neg_loglikelihood = F.binary_cross_entropy(reconstructed_x, x.view(-1,784), reduction='sum').div(x.size(0)) # Cross-entropy between outputted image and (flattened) original image\n",
        "    KL_divergence = # Fill in\n",
        "    loss = neg_loglikelihood + KL_divergence\n",
        "    return neg_loglikelihood, KL_divergence, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSgSd131fQAu"
      },
      "source": [
        "Let's write a training function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWdkM5cvq4pD"
      },
      "outputs": [],
      "source": [
        "def train(model, nr_epochs, optimizer, criterion, dataloader=trainloader, has_labels=True):\n",
        "    # use gpu if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(nr_epochs):\n",
        "        # iterate through batches\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "            # get inputs\n",
        "            if has_labels==True:\n",
        "                # get the inputs if data is a list of [inputs, labels]\n",
        "                images, labels = data\n",
        "            else:\n",
        "                images = data\n",
        "\n",
        "            # gpu\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Compute the output for all the images in the batch_size; remember we set a batch_size of 10 in the beginning!\n",
        "            reconstructed_images, mu, logvar = model(images)\n",
        "            # Compute the loss value\n",
        "            neg_loglikelihood, KL_divergence, loss = criterion(images, reconstructed_images, mu, logvar)\n",
        "            # Compute the gradients\n",
        "            loss.backward()\n",
        "            # Take the optimisation step\n",
        "            optimizer.step()\n",
        "\n",
        "        # print results for last batch\n",
        "        print(f'Epoch: {epoch:03} | ELBO loss: {loss} | KL divergence: {KL_divergence} | Negative log-likelihood: {neg_loglikelihood}')\n",
        "\n",
        "    model.to('cpu')\n",
        "    print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH8Y1zjRq77f"
      },
      "outputs": [],
      "source": [
        "vae = VAE()\n",
        "train(model=vae, nr_epochs=20, optimizer=torch.optim.Adam(vae.parameters(), lr=0.001), criterion=ELBO_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqTzQcJKnklp"
      },
      "source": [
        "Let's now sample some images from our trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJsGJvbVq_LD"
      },
      "outputs": [],
      "source": [
        "def sample_images(model, num_im):\n",
        "    '''\n",
        "    Inputs:\n",
        "        - model: trained vae\n",
        "        - num_im: int, number of images to sample\n",
        "    '''\n",
        "    # Fill in steps\n",
        "    # sample latent variables from p(z). These should be sampled from a normal distribution with mean 0 and variance 1 and return an array of size number of images x latent dimension of the encoder\n",
        "\n",
        "    # pass latent variables through the decoder\n",
        "\n",
        "    # reshape output of model into 28x28\n",
        "    images =\n",
        "\n",
        "    # plot images\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=num_im)\n",
        "    for i in range(num_im):\n",
        "        image = images[i]\n",
        "        ax[i].imshow(image.detach().numpy(), cmap='gray')\n",
        "        ax[i].axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuuoVsO5rBZ9"
      },
      "outputs": [],
      "source": [
        "sample_images(vae, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl0JcRw21htH"
      },
      "source": [
        "Run this code multiple times to see different sampled images. Note that some of the numbers generated are not easily identifiable or seem to combine more than one handwritten number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNOLf8CaWQ1V"
      },
      "source": [
        "Feel free to change the architecture (e.g. number of layers, latent dimension), batch size, number of epochs to see how this changes the model and convergence of the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG0OtNKpeoiq"
      },
      "source": [
        "# Exploring the latent space\n",
        "\n",
        "Let's try to visualise what is going on in the latent space. In this case, we have used a 2-dimensional latent space for our VAE, so we can visualise on a plane the outputs we would get by varying the values of the variables that make up the latent vector.\n",
        "\n",
        "Let $\\mathbf{z} = (x, y)$ be a latent variable and consider a possible range of values for $x$ and $y$ that we want to study. We can discretise the intervals of these ranges of values to obtain a mesh of points on the latent space. For each point $(x_i, y_j)$ on the mesh we can then apply the decoder and visualise the resulting image at the corresponding location on the plane."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHAzDzG5rHnx"
      },
      "outputs": [],
      "source": [
        "def plot_reconstructed(model, range_x, range_y, n=16):\n",
        "    '''\n",
        "    Inputs:\n",
        "    model: trained vae\n",
        "    range_x: tuple, range of values of first latent dimension\n",
        "    range_y: tuple, range of values of second latent dimension\n",
        "    n: number of images to plot along each axis\n",
        "    '''\n",
        "    w = 28 # height and width of output image\n",
        "    img = np.zeros((n*w, n*w)) # the total number of pixels for n images of size 28x28 will be n*28xn*28 pixels\n",
        "\n",
        "    # plot images in latent space\n",
        "    for i, y in enumerate(np.linspace(*range_y, n)):\n",
        "        for j, x in enumerate(np.linspace(*range_x, n)):\n",
        "            z = torch.Tensor([[x, y]])\n",
        "            x_sample = model.decode(z)\n",
        "            x_sample = x_sample.reshape(28, 28).detach().numpy()\n",
        "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_sample\n",
        "    plt.imshow(img, extent=[*range_x, *range_y], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzPvObxWrLVg"
      },
      "outputs": [],
      "source": [
        "plot_reconstructed(vae, range_x=(-3, 3), range_y=(-3, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGo2-AAz4tiM"
      },
      "source": [
        "Notice how there are regions on the plane where we can easily identify the number that we would obtain if we sampled from that region, and there are regions that interpolate or combine numbers.\n",
        "\n",
        "**Exercise:** vary the limits of range_0 and range_1 in the above function to explore the outputs of the VAE. What do you notice happens as we get close to the origin? What about as we get further away from origin?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snfgQ3-irQO8"
      },
      "source": [
        "We can also visualise where on the latent space is the latent representation of each training image. Images from the same class (which hence will be similar and have similar characteristics) will tend to have a similar encoding $\\boldsymbol{\\mu_x}$ and $\\boldsymbol{\\sigma_x}$.\n",
        "\n",
        "Let's take every image $\\mathbf{x}$ from the training set, sample a corresponding $\\mathbf{z}$ from $\\mathcal{N}(\\boldsymbol{\\mu_x}, \\text{diag}(\\boldsymbol{\\sigma_x}))$ and plot $\\mathbf{z}$ on the plane."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnF0tNc1rUnK"
      },
      "outputs": [],
      "source": [
        "def plot_latent(model):\n",
        "    for batch in trainloader:\n",
        "        # Fill in steps\n",
        "        # get images and labels in batch\n",
        "\n",
        "        # pass images through the vae encoder\n",
        "\n",
        "        # sample a latent z for each image\n",
        "        z =\n",
        "        z = z.detach().numpy() # convert to numpy array\n",
        "        # plot z values\n",
        "        scatter = plt.scatter(z[:, 0], z[:, 1], c=labels, cmap='tab10')\n",
        "    plt.legend(*scatter.legend_elements(), title=\"Label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wd3GaJgrWwp"
      },
      "outputs": [],
      "source": [
        "plot_latent(vae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA-1vX-kQ27q"
      },
      "source": [
        "Notice that:\n",
        "\n",
        "1.   All training images are encoded close to the origin\n",
        "2.   Training images from the same class seem to form clusters on the latent space\n",
        "3.   The location of the latent encoding of each class label roughly matches the region in the latent space from which we obtained that label when generating samples in the previous plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWpTi4tjriSl"
      },
      "source": [
        "# Understanding the effect of the loss on the latent encodings\n",
        "\n",
        "Recall that the loss was defined as\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}, \\phi, \\psi) = KL(q_\\psi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z})) - \\mathbb{E}_{\\mathbf{z} \\sim q_\\psi(\\mathbf{z}|\\mathbf{x})} \\log p_\\phi(\\mathbf{x}|\\mathbf{z}).\n",
        "$$\n",
        "\n",
        "In this section we aim to explore empirically how each term $KL(q_\\psi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))$ and $- \\mathbb{E}_{\\mathbf{z} \\sim q_\\psi(\\mathbf{z}|\\mathbf{x})} \\log p_\\phi(\\mathbf{x}|\\mathbf{z})$ affects the latent space and the encoding/decoding ability of the model. We will approach this by training on each of the terms in the loss separately.\n",
        "\n",
        "Note that although we may refer to the models we obtain as VAEs, they will technically **not be variational autoencoders** (even if they follow their architecture), as the definition of VAEs includes using the ELBO loss to train them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCDd05ZWbaDp"
      },
      "source": [
        "## Task: train a model using $- \\mathbb{E}_{\\mathbf{z} \\sim q_\\psi(\\mathbf{z}|\\mathbf{x})} \\log p_\\phi(\\mathbf{x}|\\mathbf{z})$ as the loss\n",
        "\n",
        "You may want to either modify the train function or write a different loss function to input as criterion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_aXfwIrYQGm"
      },
      "outputs": [],
      "source": [
        "vae2 = VAE()\n",
        "train(model=vae2, nr_epochs=20, optimizer=torch.optim.Adam(vae2.parameters(), lr=0.001), criterion=ELBO_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t99khwgMZDNH"
      },
      "source": [
        "After training, use the plot_latent function to visualise the encodings of the training set. How does the plot differ from what we observed previously?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCILnmShZVs6"
      },
      "outputs": [],
      "source": [
        "plot_latent(vae2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTJfZkvxOg7I"
      },
      "source": [
        "Let's try sampling some images. What do you observe? Can you generate all numbers?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2sqeiaFtHwp"
      },
      "outputs": [],
      "source": [
        "sample_images(vae2, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL0dvt6sPGaV"
      },
      "source": [
        "You can also use the plot_reconstructed function to visualise the outputs. Feel free to play around with the ranges for $x$ and $y$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V1IBdVAtOyG"
      },
      "outputs": [],
      "source": [
        "plot_reconstructed(vae2, range_x=(-3, 3), range_y=(-3, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1EKrpUUZecW"
      },
      "source": [
        "## Task: train a model using $KL(q_\\psi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))$ as the loss\n",
        "\n",
        "Similar to the previous task, now train a VAE only on the KL-divergence term of the loss. As before, you may want to modify the train function or write a new loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSzsmj-Aa3KW"
      },
      "outputs": [],
      "source": [
        "vae3 = VAE()\n",
        "train(model=vae3, nr_epochs=10, optimizer=torch.optim.Adam(vae3.parameters(), lr=0.001), criterion=ELBO_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npt-vm8Bsefi"
      },
      "source": [
        "Again, use the plot_latent function to visualise the encodings of the training set. How is this plot different to what you have previously observed?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KewoIyT1bRLS"
      },
      "outputs": [],
      "source": [
        "plot_latent(vae3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVCN2LVydYhF"
      },
      "source": [
        "Try sampling new images. Is the model capable of decoding anything meaningful from the latent space?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkymWPFZdhgH"
      },
      "outputs": [],
      "source": [
        "sample_images(vae3, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvWfnvHj9PRF"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "Based on the observations from the previous tasks, can you explain how each term in the ELBO loss contributes to shaping the latent space?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EswIenudwnYn"
      },
      "source": [
        "# Convolutional variational auto-encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14PoA2PhFEkq"
      },
      "source": [
        "We will now look at implementing a convolutional variational autoencoder. We will work with a dataset of images containing 2 non-overlapping randomly coloured dots. The images are of size $64 \\times 64$ (with 3 channels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13-kRNJXDJfi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "\n",
        "class DotsDataset(Dataset):\n",
        "    def __init__(self, db_path, noisy=True):\n",
        "        self.db_path = db_path\n",
        "        self.db_files = [os.listdir(path) for path in db_path]\n",
        "        self.noisy = noisy\n",
        "        self.images = None\n",
        "\n",
        "    def load_data(self):\n",
        "        images_list = []\n",
        "        # iterate through paths in input list\n",
        "        for path in self.db_path:\n",
        "            # iterate through files in path\n",
        "            for file in self.db_files[self.db_path.index(path)]:\n",
        "                filename_path = os.path.join(path, file)\n",
        "                # load images\n",
        "                img = np.load(filename_path)['images']\n",
        "                # noise\n",
        "                if self.noisy:\n",
        "                    img += np.random.normal(loc=0, scale=0.03, size=img.shape)\n",
        "                    img = 1.0 - np.abs(1.0 - img)\n",
        "                    img = np.abs(img)\n",
        "                # append images to list\n",
        "                images_list.append(img)\n",
        "        # convert to tensor and adjust dimensions\n",
        "        image_tensor = torch.from_numpy(np.concatenate(images_list, axis=0))\n",
        "        self.images = torch.movedim(image_tensor, 3, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.images is None:\n",
        "            self.load_data()\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.images is None:\n",
        "            self.load_data()\n",
        "        image = self.images[idx]\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZrQ6NueFm1R"
      },
      "source": [
        "Let's load the dataset. Make sure you have the 2_dots folder in your directory before running this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81McJbhLDmi1",
        "outputId": "250594c2-49cb-4ee8-e13d-adebd6b8c235"
      },
      "outputs": [],
      "source": [
        "train_set_dots = DotsDataset(db_path=['./2_dots'], noisy=False)\n",
        "print('Size of dataset:',train_set_dots.__len__())\n",
        "\n",
        "trainloader_dots = torch.utils.data.DataLoader(train_set_dots, batch_size=128, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns0IkSPT2XeW"
      },
      "source": [
        "Note: if you are using Google Colab and you get an error when loading the dataset related to ./2_dots/.ipynb_checkpoints uncomment and run this cell and then try again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Qch686iWam"
      },
      "outputs": [],
      "source": [
        "#rmdir ./2_dots/.ipynb_checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jACU87ciFsDn"
      },
      "source": [
        "We can visualise one of the images in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "OaJeUI1rF4DQ",
        "outputId": "6d2abacc-03a2-43db-c92a-45283958f4cf"
      },
      "outputs": [],
      "source": [
        "# get some random training images; this will load a set of images of size batchsize\n",
        "dataiter = iter(trainloader_dots)\n",
        "images = next(dataiter)\n",
        "img = images[0] # take just the first image; the above thing is an array of images of size batchsize\n",
        "\n",
        "# Print the original image\n",
        "npimg = img.numpy() # translate to numpy\n",
        "npimg_t = np.transpose(npimg, (1, 2, 0))\n",
        "plt.imshow(npimg_t)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsBIQFJXKDsg"
      },
      "source": [
        "We will now train a convolutional variational autoencoder on the 2_dots dataset. The structure of the network will be the same as for the MNIST dataset with the additional use of convolutional layers. However, the decoder network will be slightly different.\n",
        "\n",
        "**Decoder**\n",
        "\n",
        "We previosly assumed that we could model $p_\\phi(\\mathbf{x}|\\mathbf{z})$ using a Bernoulli distribution. However, in this case the pixels in general will not take only two possible values, so instead we will assume a Gaussian distribution.\n",
        "\n",
        "$$\n",
        "p_{\\phi}(\\mathbf{x}|\\mathbf{z}) = \\left(2\\pi\\right)^{-D/2}det(\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\Sigma(\\mathbf{x}-\\boldsymbol{\\mu})\\right)\n",
        "$$\n",
        "\n",
        "We would then need (in a similar manner as with the encoder) to output a mean and covariance for each latent variable $\\mathbf{z}$ passed through the decoder. However, we typically assume the covariance matrix is the identity matrix (and hence don't parameterise it). In order to calculate the term $- \\mathbb{E}_{\\mathbf{z} \\sim q_\\psi(\\mathbf{z}|\\mathbf{x})} \\log p_\\phi(\\mathbf{x}|\\mathbf{z})$ from the loss, we can then take a Monte Carlo approximation of the expectation - this is typically done with only one sample, (typically chosen to be the outputted mean $\\boldsymbol{\\mu}$ instead of randomly sampling from $\\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{I})$).\n",
        "\n",
        "Hence, $- \\mathbb{E}_{\\mathbf{z} \\sim q_\\psi(\\mathbf{z}|\\mathbf{x})} \\log p_\\phi(\\mathbf{x}|\\mathbf{z}) \\propto - \\|\\mathbf{x} - \\boldsymbol{\\mu}\\|^2$. As this term is comparing the difference between the output $\\boldsymbol{\\mu}$ of the decoder and the original image $\\mathbf{x}$, this term is also known as the reconstruction loss. In practice, most models implement $- \\mathbb{E}_{\\mathbf{z} \\sim q_\\psi(\\mathbf{z}|\\mathbf{x})} \\log p_\\phi(\\mathbf{x}|\\mathbf{z})$ as $\\|\\mathbf{x} - \\boldsymbol{\\mu}\\|^2$ (e.g. by using F.mse_loss), ignoring any constants that would scale the term.\n",
        "\n",
        "Let's define the ELBO loss for the images under our new assumptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdu8w7sPyREk"
      },
      "outputs": [],
      "source": [
        "def ELBO_loss_Gaussian(x, reconstructed_x, mu, logvar):\n",
        "    '''\n",
        "    calculates ELBO loss when p(x|z) is assumed to be Gaussian\n",
        "    Inputs:\n",
        "        - x: batch of images\n",
        "        - reconstructed_x: batch of reconstructed images\n",
        "        - mu: batch of latent mean values\n",
        "        - logvar: batch of latent logvariances\n",
        "    Outputs:\n",
        "        - neg_loglikelihood: average value of negative log-likelihood term across batch\n",
        "        - KL_divergence: average value of KL divergence term across batch\n",
        "        - loss: average ELBO loss across batch\n",
        "    '''\n",
        "    neg_loglikelihood = # Fill in\n",
        "    KL_divergence = # Fill in\n",
        "    loss = neg_loglikelihood + KL_divergence\n",
        "    return neg_loglikelihood, KL_divergence, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7nuI-uFbvap"
      },
      "source": [
        "**Transpose convolutions**\n",
        "\n",
        "In order to upscale the images when passed through the decoder, we need to upscale the inputs that we get after downscaling in the encoder with convolutional layers (nn.Conv2d) has on an input. The operator used for this is a transposed convolutional operator (see https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html). See https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md for a visualisation of how convolutional layers and transposed convolutional layers are applied.\n",
        "\n",
        "Note that this is not exaclty a deconvolutional layer (i.e. it is not the inverse of a convolutional layer). The inputs to nn.ConvTranspose2d are the same as in a convolutional layer:\n",
        "\n",
        "*   in_channels: number of input channels\n",
        "*   out_channels: number of output channels\n",
        "*   kernel_size: size of the filter\n",
        "*   stride: number of rows/columns traversed every time the filter is applied\n",
        "*   padding: size of extra pixels added at edge of image\n",
        "\n",
        "As with convolutional layers, it is useful to keep track of the dimension of the outputs after each transposed convolution is applied. Assuming we are working with square inputs, let us define the following:\n",
        "\n",
        "Let us define the following:\n",
        "\n",
        "*   $n_{in}$: height (or width) of input image\n",
        "*   $k$: size of kernel\n",
        "*   $p$: size of padding\n",
        "*   $s$: size of stride\n",
        "\n",
        "The height of the output image will be given by:\n",
        "$$\n",
        "n_{out} = (n_{in} - 1)s - 2p + k\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APLUX7KcFibJ"
      },
      "source": [
        "## Task : build a convolutional variational autoencoder\n",
        "\n",
        "Choose the architecture of the encoder and decoder of the VAE. You may choose the number of layers, arguments inputted to the convolutional layers (kernel size, stride, padding), activation function, ... The layers should be added inside the nn.Sequential (https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) in the encoder and decoder in the build_VAE function. The encoder should contain some convolutional layers, followed by linear layers. The decoder should contain some linear layers, followed by transposed convolution layers.\n",
        "\n",
        "Tips:\n",
        "*   Keep track of the dimension of the output after each layer\n",
        "*   Don't make your model too large to avoid taking too long to train - you should be able to obtain some results with no more than 4 convolutional layers, 3 linear layers in the encoder (and a similar number for the decoder with transposed convolutions instead)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFKVDPQexrR1"
      },
      "outputs": [],
      "source": [
        "# We will use nn.Sequential to build the encoders and decoders - we will use the View class to resize the images\n",
        "class View(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super(View, self).__init__()\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        return tensor.view(self.size) # used to resize tensor to self.size\n",
        "\n",
        "\n",
        "class CVAE(nn.Module):\n",
        "    def __init__(self, d=20):\n",
        "        \"\"\"builds VAE\n",
        "        Inputs:\n",
        "            - d: dimension of latent space\n",
        "        \"\"\"\n",
        "        super(CVAE, self).__init__()\n",
        "        self.d = d\n",
        "        # Build VAE here\n",
        "        self.encoder, self.decoder, self.encoder_mean, self.encoder_lv = self.build_VAE(d)\n",
        "\n",
        "    def build_VAE(self, d):\n",
        "        \"\"\"builds VAE with specified latent dimension and number of layers\n",
        "        Inputs:\n",
        "            -d: latent dimension\n",
        "        \"\"\"\n",
        "        # Input size: 64x64\n",
        "        encoder = nn.Sequential(\n",
        "            # Convolutional layers - args: in_channels, out_channels, kernel_size, stride, padding\n",
        "            nn.Conv2d(3, 32, 4, 2, 1), # example of how your encoder could start - you may want to change this\n",
        "            nn.ReLU(True),\n",
        "            # Fill in\n",
        "\n",
        "            # Flatten images\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # Linear layers\n",
        "            # Fill in\n",
        "\n",
        "        )\n",
        "\n",
        "        encoder_mean = nn.Linear( , d)\n",
        "        encoder_lv = nn.Linear( , d)\n",
        "\n",
        "        decoder = nn.Sequential(\n",
        "            # Linear layers\n",
        "            # Fill in\n",
        "\n",
        "            # Reshape images\n",
        "            View((-1, , , )), # args should be (-1, num_channels, n, n) where you want to resize to images of dimension nxn\n",
        "            # Transposed convolution layers - args: in_channels, out_channels, kernel_size, stride, padding\n",
        "            # Fill in - make sure the output size is 64x64. Do not pass your final output through any activation functions\n",
        "\n",
        "        )\n",
        "\n",
        "        return encoder, decoder, encoder_mean, encoder_lv\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"take an image, and return latent space mean + log variance\n",
        "        Inputs:\n",
        "            -images, x\n",
        "        Outputs:\n",
        "            -means in latent dimension\n",
        "            -logvariances in latent dimension\n",
        "        \"\"\"\n",
        "        h1 = self.encoder(x)\n",
        "        return self.encoder_mean(h1), self.encoder_lv(h1)\n",
        "\n",
        "    def reparametrise(self, mu, logvar):\n",
        "        \"\"\"Sample in latent space according to mean and logvariance\n",
        "        Inputs:\n",
        "            -mu: batch of means\n",
        "            -logvar: batch of logvariances\n",
        "        Outputs:\n",
        "            -samples: batch of latent samples\n",
        "        \"\"\"\n",
        "        # Fill in\n",
        "        return samples\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"Decode latent space samples\n",
        "        Inputs:\n",
        "            -z: batch of latent samples\n",
        "        Outputs:\n",
        "            -x_recon: batch of reconstructed images\n",
        "        \"\"\"\n",
        "        raw_out = self.decoder(z)\n",
        "        x_recon = torch.sigmoid(raw_out) # pass raw output through sigmoid activation function\n",
        "        return x_recon\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Do full encode and decode of images\n",
        "        Inputs:\n",
        "            - x: batch of images\n",
        "        Outputs:\n",
        "            - batch of reconstructed images\n",
        "            - mu: batch of latent mean values\n",
        "            - logvar: batch of latent logvariances\n",
        "        \"\"\"\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparametrise(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7UTuG0oIRKB"
      },
      "source": [
        "Now train your model. It can sometimes be hard to find the right hyperparameters for this. There are some suggested parameters for training with Adam in the cell below, but these will have to be adjusted to your model, so feel free to change them or even change the optimizer used. Also adjust the number of epochs appropriately. See if you can get the ELBO loss below 50, but if the model is taking too long to train you can still use it to sample some images and look at what the model has learnt (see the sample_dots function).\n",
        "\n",
        "Tips:\n",
        "*   If during training the KL divergence is very small ($\\approx \\times 10^{-7}$) and does not increase, try increasing your learning rate\n",
        "*   Use an optimiser with an adaptive learning rate (e.g. Adam)\n",
        "*   Use a GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpBg_Bbq-BrX"
      },
      "outputs": [],
      "source": [
        "conv_vae = CVAE()\n",
        "train(model=conv_vae, nr_epochs=80, optimizer=torch.optim.Adam(conv_vae.parameters(), lr=0.0006, betas=(0.4, 0.9)), criterion=ELBO_loss_Gaussian, dataloader=trainloader_dots, has_labels=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVFGcl2u_xIQ"
      },
      "source": [
        "Let's now sample some images from the VAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-rB3QHq_3O9"
      },
      "outputs": [],
      "source": [
        "def sample_dots(model, num_im):\n",
        "    '''\n",
        "    Inputs:\n",
        "        - model: trained vae\n",
        "        - num_im: int, number of images to sample\n",
        "    '''\n",
        "    # Fill in steps\n",
        "    # get latent dimension of model\n",
        "\n",
        "    # sample latent vectors from p(z)\n",
        "\n",
        "    # adjust dimensions to plot\n",
        "    output =\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=num_im, figsize=(48, 6))\n",
        "\n",
        "    for i in range(num_im):\n",
        "        image = output[i]\n",
        "        ax[i].imshow(image.detach().numpy())\n",
        "        #ax[i].axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WOG3uLwJYnj"
      },
      "outputs": [],
      "source": [
        "sample_dots(conv_vae, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQjnLorgZ80R"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "\n",
        "*   Diederik P Kingma and Max Welling (2013). Auto-Encoding Variational Bayes. Arxiv: https://arxiv.org/pdf/1312.6114.pdf\n",
        "*   Gabriel Loaiza-Ganem and John P. Cunningham (2019). The continuous Bernoulli: fixing a pervasive error in\n",
        "variational autoencoders. Arxiv: https://arxiv.org/pdf/1512.03385.pdf\n",
        "*   https://lilianweng.github.io/posts/2018-08-12-vae/\n",
        "*   https://www.tensorflow.org/tutorials/generative/cvae\n",
        "*   https://medium.com/towards-data-science/intuitively-understanding-variational-autoencoders-1bfe67eb5daf#:~:text=Variational%20Autoencoders%20(VAEs)%20have%20one,easy%20random%20sampling%20and%20interpolation.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
